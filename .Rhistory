rm(list = ls())
# For now, test that your submission works with random test responses.
n.test = 1300
test.labels = sample(12, n.test, replace = T)
# Read in the predictions file.
# You may need to change the file path here.
z = read.table("z.txt")
# This is your error rate. Hopefully this number will
# get smaller when we use non-random test responses!
mean((z - test.labels)^2)
lm.ridge
plot(lm.ridge)
plot(lm.lasso)
plot(glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], alpha=1))
lm.lasso <- glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], alpha=1)
library(MASS)
library(genridge)
library(mcmc)
library(glmnet)
library(boot)
set.seed(23)
# setwd('~shifan/Dropbox/STANFORD/15Winter/STATS315a/hw3/')
# Load training and test samples
train <- read.csv("wine.train.csv", header=TRUE)
test <- read.csv("wine.test.ho.csv", header=TRUE)
setwd('~shifan/Dropbox/STANFORD/15Winter/STATS315a/hw3/')
# Load training and test samples
train <- read.csv("wine.train.csv", header=TRUE)
test <- read.csv("wine.test.ho.csv", header=TRUE)
setwd('~shifan/Dropbox/STANFORD/15Winter/STATS315a/hw3/wine')
# Load training and test samples
train <- read.csv("wine.train.csv", header=TRUE)
test <- read.csv("wine.test.ho.csv", header=TRUE)
# Encode categorial data
train <- train[,2:ncol(train)]
test <- test[,2:ncol(test)]
train[,1] <- model.matrix(~train[,1])[,2]
test[,1] <- model.matrix(~test[,1])[,2]
p = ncol(train) - 1; # Number of features
# Ridge regression
lambda <- 10^seq(-5,5,0.1)
model <- ridge(train$quality~., train[,1:p], lambda = lambda)
traceplot(model,X="df")
lm.ridge <- glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], lambda=lambda, alpha=0)
cv.ridge.err=cv.glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], lambda=lambda, alpha=0)
plot(cv.ridge.err)
plot(lm.ridge)
lm.lasso <- glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], alpha=1)
plot(cv.lasso.err)
lasso.opt <- glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], lambda=10^(-6.45),alpha=1)
predict(lasso.opt,model.matrix(test))
predict.glmnet(lasso.opt,model.matrix(test))
predict(lasso.opt,newx=model.matrix(test))
test
lasso.opt <- glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], lambda=10^(-5),alpha=1)
predict(lasso.opt,newx=test)
predict(lasso.opt,newx=as.matrix(test))
lm.lasso <- glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], alpha=1)
lm.lasso <- glmnet(train[,1:p],train[,p+1], alpha=1)
lm.lasso <- glmnet(model.matrix(train[,1:p]),train[,p+1], alpha=1)
predict(lasso.opt,newx=model.matrix(~.,test))
best.subset.regression <- function(formula, train.data, subset.size, method) {
leaps <- summary(regsubsets(formula, data=train.data,nvmax=subset.size,method=method,really.big=TRUE));
#print(subset.size);
#print(leaps$which)
best.subset <- leaps$which[subset.size,2:13];
return(lm(quality ~ ., data = train.data[,c(best.subset,TRUE)]));
}
print('BEST SUBSET')
subset.scores <- lapply(1:12, function(y) {cv.error(function(x) {return(best.subset.regression(quality ~ ., x, y, "exhaustive"))}, 10, wine.train, function(model, x) predict(model, x[,1:12]))})
print(subset.scores)
best.subset.regression <- function(formula, train.data, subset.size, method) {
leaps <- summary(regsubsets(formula, data=train.data,nvmax=subset.size,method=method,really.big=TRUE));
#print(subset.size);
#print(leaps$which)
best.subset <- leaps$which[subset.size,2:13];
return(lm(quality ~ ., data = train.data[,c(best.subset,TRUE)]));
}
subset.scores <- lapply(1:12, function(y) {cv.error(function(x) {return(best.subset.regression(quality ~ ., x, y, "exhaustive"))}, 10, wine.train, function(model, x) predict(model, x[,1:12]))})
print(subset.scores)
cv.error <- function(model.fn, num.folds, train.data, predict.fn) {
N = nrow(train.data);
fold.size <- floor(N/num.folds);
error <- 0.0;
fold.begin <- 1; fold.end <- floor(N/num.folds);
for (i in 1:num.folds) {
model.i <-  model.fn(train.data[-(fold.begin:fold.end),]);
# add in i^th fold training error
predict.i <- predict.fn(model.i, train.data[fold.begin:fold.end,]);
error <- error + sum((predict.i-train.data[fold.begin:fold.end,]$quality)^2)/fold.size; #rss
#error <- error + mean(round(predict.i)!=train.data[fold.begin:fold.end,]$quality); #accuracy
# update fold.begin and fold.end
fold.begin <- fold.end + 1;
fold.end <- fold.begin + fold.size - 1;
}
return(error/num.folds);
}
subset.scores <- lapply(1:12, function(y) {cv.error(function(x) {return(best.subset.regression(quality ~ ., x, y, "exhaustive"))}, 10, wine.train, function(model, x) predict(model, x[,1:12]))})
wine.train = train
subset.scores <- lapply(1:12, function(y) {cv.error(function(x) {return(best.subset.regression(quality ~ ., x, y, "exhaustive"))}, 10, wine.train, function(model, x) predict(model, x[,1:12]))})
library(leaps);
install.packages("leaps")
library(leaps);
subset.scores <- lapply(1:12, function(y) {cv.error(function(x) {return(best.subset.regression(quality ~ ., x, y, "exhaustive"))}, 10, wine.train, function(model, x) predict(model, x[,1:12]))})
subset.scores
plot(subset.scores)
subset.scores
plot(subset.scores)
plot(1:12,subset.scores)
subset.scores <- lapply(1:12, function(y) {cv.error(function(x) {return(best.subset.regression(quality ~ ., x, y, "exhaustive"))}, 10, wine.train, function(model, x) predict(model, x[,1:12]))})
best.subset.regression <- function(formula, train.data, subset.size, method) {
leaps <- summary(regsubsets(formula, data=train.data,nvmax=subset.size,method=method));
best.subset <- leaps$which[subset.size,2:13];
return(lm(quality ~ ., data = train.data[,c(best.subset,TRUE)]));
}
subset.scores <- lapply(1:12, function(y) {cv.error(function(x) {return(best.subset.regression(quality ~ ., x, y, "exhaustive"))}, 10, wine.train, function(model, x) predict(model, x[,1:12]))})
plot(1:12,subset.scores)
regsubsets((x=model.matrix(~ ., train[,1:p]), y=train[,p+1], weights=rep(1, length(y)), nbest=1, nvmax=8,force.in=NULL, force.out=NULL, intercept=TRUE, method=c("exhaustive","backward", "forward", "seqrep"), really.big=FALSE))
regsubsets((x=model.matrix(~ ., train[,1:p]), y=train[,p+1], weights=rep(1, length(y)),nbest=1,nvmax=8,force.in=NULL, force.out=NULL, intercept=TRUE, method=c("exhaustive","backward", "forward", "seqrep"), really.big=FALSE))
subset.size=1
leaps <- summary(regsubsets(quality~., data=model.matrix(~ ., train[,1:p]),nvmax=subset.size,method=method,really.big=TRUE))
subset.size=1
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
subset.size=1
method='exaustive'
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
subset.size=1
method='exhaustive'
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
leaps
subset.size=12
method='exhaustive'
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
leaps
leaps$which
leaps$which[subset.size,]
leaps$which[subset.size,2:13]
leaps$which[subset.size,2:p]
leaps$which[subset.size,2:p+1]
best.subset <- leaps$which[subset.size,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
subset.size=1:12
method='exhaustive'
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
leaps
best.subset <- leaps$which[subset.size,2:p+1]
leaps
leaps
subset.size=12
method='exhaustive'
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
leaps
leaps$which
leaps$which[1,2:p+1]
p
best.subset <- leaps$which[1:subset.size,2:p+1]
best.subset
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
for (subset.size in 1:p){
best.subset <- leaps$which[subset.size,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
}
for (subset.size in 1:2){
best.subset <- leaps$which[subset.size,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
}
subset.size=1
best.subset <- leaps$which[subset.size,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
subset.size=p
method='exhaustive'
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
subset.size=1
best.subset <- leaps$which[subset.size,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
best.subset
c(best.subset,TRUE)
subset.size=p
method='exhaustive'
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
leaps
best.subset <- leaps$which[subset.size,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
best.subset <- leaps$which[1,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
best.subset <- leaps$which[12,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
best.subset <- leaps$which[1,2:p+1]
best.subset
best.subset <- leaps$which[2,2:p+1]
best.subset
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
best.subset <- leaps$which[5,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
for (subset.size in 1:p){
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
}
subset.size = p
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
subset.size = 5
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
leaps
leaps$which[p,2:p+1]
leaps$which[5,2:p+1]
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
c(best.subset,TRUE)
subset.size
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
lm(quality ~ ., data = train[,c(best.subset,TRUE)])
train[,c(best.subset,TRUE)]
test <- train[,c(best.subset,TRUE)]
nrow(test)
ncol(test)
best.subset
test(,5)
train(,5)
train
train(,5)
train[,5]
best.subset
train[,c(best.subset,TRUE)]
test <- train[,c(best.subset,TRUE)]
ncol(test)
test[,5]
test
test[1:5,]
best.subset
data = train[,c(best.subset,TRUE)])
data = train[,c(best.subset,TRUE)]
data[1:5,]
best.subset
test <- train[,best.subset]
test[1:5,]
test <- train[,best.subset==TRUE]
test[1:5,]
best.subset
best.subset==TRUE
train[1,1]
train[0,1]
factor(train)
head(train)
data.frame(train)
rownames(train)
colnames(train)
subset.size = 5
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
c(best.subset,TRUE)
data = train[,c(best.subset,TRUE)]
ncol(data)
subset.size
subset.size = 5
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
lm(colnames(train)[subset.size] ~ ., data = train[,c(best.subset,TRUE)])
colnames(train)[subset.size]
colnames(train)[subset.size+1]
data[1:5,]
subset.data = train[,c(best.subset,TRUE)]
lm(colnames(subset.data)[subset.size] ~ ., data = subset.data)
colnames(subset.data)
colnames(subset.data)[subset.size]
subset.data
lm(colnames(subset.data)[subset.size] ~ ., data = subset.data)
nrow(subset.data)
subset.data[1:5,]
lm(colnames(subset.data)[subset.size] ~ ., data = subset.data)
colnames(subset.data)[subset.size]
subset.size = 5
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
subset.data
nrow(subset.data)
ncol(subset.data)
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
plot(cv.lasso.err)
cv.lasso.err[1]
cv.lasso.err$lambda.1se
cv.lasso.err$lambda.min
cv.lasso.err$cvm
cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
subset.size = 5
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
cv.lasso.err$lambda.min
array(0,c(3,5))
array(0,c(1,5))
x <- array(0,c(1,5))
x[1]=5
x
method='exhaustive'
subset.error <- array(0,c(1,p))
for (subset.size in 1:p){
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
subset.error[subset.size] <- cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
}
for (subset.size in 1:p){
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
# subset.error[subset.size] <- cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
}
subsett.size=3
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
subset.size = 5
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
subset.size = 3
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
subset.size = p
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
for (subset.size in 1:p){
# subset.size = p
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
}
subset.size = 1
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
subset.size = 2
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
subset.size = 3
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
lm.lasso <- glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., subset.data[,1:subset.size-1]),train[,subset.size], alpha=1)
cv.lasso.err$cvm[cv.lasso.err$lambda == cv.lasso.err$lambda.min]
subset.size = 3
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
best.subset
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset.data = train[,c(best.subset,TRUE)]
subset.data = train[,best.subset]
subset = train[,best.subset]
subset[1:5,]
best.subset
subset = train[,best.subset]
lm <- glm.fit(subset,train$quality)
lm
cv.err <- cv.glm(train, lm, K = 10)$delta
lm <- glm.fit(subset,train$quality)
lm
cv.err <- cv.glm(train, lm, K = 10)$delta
train
train[1:5,]
train[1:5,c(1,4,5)]
best.subset
is.logical(best.subset)
as.logical(best.subset)
as.numeric(best.subset)
which(best.subset==TRUE)
which(as.numeric(best.subset)==1)
c(which(as.numeric(best.subset)==1),p+1)
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset = train[,c(which(as.numeric(best.subset)==1),p+1)
)
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset = train[,c(which(as.numeric(best.subset)==1),p+1)]
glm(quality~., data=subset)
lm <- glm(quality~., data=subset)
cv.err <- cv.glm(subset, lm, K = 10)$delta
cv.err
subset.error <- array(0,c(1,p))
for (subset.size in 1:p){
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset = train[,c(which(as.numeric(best.subset)==1),p+1)]
lm <- glm(quality~., data=subset)
cv.err <- cv.glm(subset, lm, K = 10)$delta
subset.error <- cv.err[2]
}
subset.error
subset.error <- array(0,c(1,p))
for (subset.size in 1:p){
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset = train[,c(which(as.numeric(best.subset)==1),p+1)]
lm <- glm(quality~., data=subset)
cv.err <- cv.glm(subset, lm, K = 10)$delta
subset.error[subset.size] <- cv.err[2]
}
subset.error
plot(1:p,subset.error)
plot(1:p,subset.error,type="o")
xlab="Subset Size", ylab="CV Error")
plot(1:p,subset.error,type="o",xlab="Subset Size", ylab="CV Error")
method='exhaustive'
subset.error <- array(0,c(1,p))
for (subset.size in 1:p){
leaps <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method,really.big=TRUE))
best.subset <- leaps$which[subset.size,2:p+1]
subset = train[,c(which(as.numeric(best.subset)==1),p+1)]
lm <- glm(quality~., data=subset)
cv.err <- cv.glm(subset, lm, K = 10)$delta
subset.error[subset.size] <- cv.err[2]
}
plot(1:p,subset.error,type="o",xlab="Subset Size", ylab="CV Error")
fit <- princomp(train, cor=TRUE)
summary(fit)
loadings(fit)
plot(fit,type='lines')
fit$scores
biplot(fit)
loadings(fit)
plot(pca,type="-")
pca <- princomp(train, cor=TRUE)
summary(pca) # print variance accounted for
plot(pca,type="-")
# biplot(fit)
# Q1 ESL18.9
# The optimal seperating hyperplane makes the wider margin, since the minimal norm linear-square solution has more stricted conditions that linear square solution with all zero residuals. So the maximimal distance that the minimal norm can reach is not wider than that of optimal seperating hyperplane.
library('e1071')
set.seed(1)
x = matrix(c(rnorm(10*30,-1,1), rnorm(10*30,1,1)), nrow = 20, byrow = TRUE)
y = rep(c(-1,1), each = 10)
# minimal norm solution in linear square solutions
model_0 = svd(x)
beta_0 = model_0$v %*% diag(1/model_0$d) %*% t(model_0$u) %*% y
norm_0 = sqrt(t(beta_0) %*% beta_0)
project_data_0 = c(x %*% beta_0) / norm_0
# optimal seperating hyperplane (i.e. linear kernel SVM)
model = svm(x, y, scale = FALSE, kernel = "linear")
beta = t(model$SV) %*% model$coefs
norm = sqrt(t(beta) %*% beta)
project_data = c(x %*% beta) / norm
# plot
par(mfrow = c(1,2))
plot(project_data_0, ylim = c(-6,6), xlab = "data ID", ylab = "projected data coordinate", main = "minimal norm solution")
plot(project_data, ylim = c(-6,6), xlab = "data ID", ylab = "projected data coordinate", main = "optimal seperating hyperplane")
# Conclusion
# From the figure, we can see that the distance between projected data in the optimal seperating hyperplane case is wider than that in the minimal norm linear-square solution case.
library(svm)
install.packages("e1071")
detach("package:boot", unload=TRUE)
detach("package:car", unload=TRUE)
# Q1 ESL18.9
# The optimal seperating hyperplane makes the wider margin, since the minimal norm linear-square solution has more stricted conditions that linear square solution with all zero residuals. So the maximimal distance that the minimal norm can reach is not wider than that of optimal seperating hyperplane.
library('e1071')
set.seed(1)
x = matrix(c(rnorm(10*30,-1,1), rnorm(10*30,1,1)), nrow = 20, byrow = TRUE)
y = rep(c(-1,1), each = 10)
# minimal norm solution in linear square solutions
model_0 = svd(x)
beta_0 = model_0$v %*% diag(1/model_0$d) %*% t(model_0$u) %*% y
norm_0 = sqrt(t(beta_0) %*% beta_0)
project_data_0 = c(x %*% beta_0) / norm_0
# optimal seperating hyperplane (i.e. linear kernel SVM)
model = svm(x, y, scale = FALSE, kernel = "linear")
beta = t(model$SV) %*% model$coefs
norm = sqrt(t(beta) %*% beta)
project_data = c(x %*% beta) / norm
# plot
par(mfrow = c(1,2))
plot(project_data_0, ylim = c(-6,6), xlab = "data ID", ylab = "projected data coordinate", main = "minimal norm solution")
plot(project_data, ylim = c(-6,6), xlab = "data ID", ylab = "projected data coordinate", main = "optimal seperating hyperplane")
# Conclusion
# From the figure, we can see that the distance between projected data in the optimal seperating hyperplane case is wider than that in the minimal norm linear-square solution case.
