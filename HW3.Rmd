---
title: "Stats 315 Homework 3"
author: "Shifan Mao, Chen Xue"
date: "March 6, 2016"
output:
  pdf_document:
    number_sections: true
---

\section{Problem 1 (ESL18.9)}

The optimal seperating hyperplane makes the wider margin, since the minimal norm linear-square solution has more stricted conditions that linear square solution with all zero residuals. So the maximimal distance that the minimal norm can reach is not wider than that of optimal seperating hyperplane.

We first initialize the datapoints (x,y).

```{r message=FALSE}
library('e1071')
set.seed(1)
x = matrix(c(rnorm(10*30,-1,1), rnorm(10*30,1,1)), nrow = 20, byrow = TRUE)
y = rep(c(-1,1), each = 10)
```

Then we solve minimal norm solution in linear square solutions

```{r}
model_0 = svd(x)
beta_0 = model_0$v %*% diag(1/model_0$d) %*% t(model_0$u) %*% y
norm_0 = sqrt(t(beta_0) %*% beta_0)
project_data_0 = c(x %*% beta_0) / norm_0
```

Next we look for theoptimal seperating hyperplane (i.e. linear kernel SVM)

```{r}
model = svm(x, y, scale = FALSE, kernel = "linear")
beta = t(model$SV) %*% model$coefs
norm = sqrt(t(beta) %*% beta)
project_data = c(x %*% beta) / norm
```

To make a comparison, we plot the projected data coordinates of dataset with different response labels ordered by data ID.

```{r, echo=FALSE, fig.width = 8, fig.height = 3}
par(mfrow = c(1,2))
plot(project_data_0, ylim = c(-6,6), xlab = "data ID", ylab = "projected data coordinate", main = "minimal norm solution")
plot(project_data, ylim = c(-6,6), xlab = "data ID", ylab = "projected data coordinate", main = "optimal seperating hyperplane")
```

In conclusion, from the figure we can see that the distance between projected data in the optimal seperating hyperplane case is wider than that in the minimal norm linear-square solution case.

\clearpage
\section{Problem 3}

We initialize (x,y) dataset.

```{r message=FALSE}
library('e1071')
library('glmnet')
set.seed(5)
x = matrix(c(rnorm(25*2,mean=-1,sd=1),rnorm(25*2,mean=1,sd=1)), nrow = 50, byrow = TRUE)
y = rep(c(-1,1), each = 25)
```

We first find the optimal seperating hyperplane (i.e. linear kernel SVM)

```{r}
model = svm(x, factor(y), kernel = "linear", cost = 1e4)
beta = t(model$SV) %*% model$coefs
norm = sqrt(t(beta) %*% beta)
beta_normalized = c(beta) / norm
```

Then we implment ridge regression

```{r}
# lambda = exp(seq(from = 0, to = log(1e-5), length = 300))
lambda <- 10^seq(from=5,to=-5,by=-0.1)
model2 = glmnet(x, y, alpha=0, standardize=FALSE, family="binomial", lambda=lambda)
beta2 = coef(model2)[-1,]
norm2 = sqrt(diag(t(beta2)%*%beta2))
beta2_normalized = scale(beta2, center = FALSE, scale = norm2)
```

To make a comparison, we plot the quantity $|\langle \theta_{\lambda}, \beta_{svm} \rangle|$ with $\lambda$ approaching $0$.

```{r}
inner_product = t(beta_normalized)%*%beta2_normalized
par(mfrow = c(1,1))
plot(lambda, abs(inner_product), log="x", xlab = expression(lambda), ylab = "|Inner product|")
```

In conclusion, the estimated beta of ridge logistic regression is orthogonal to the separating hyperplane when lambda approaches to 0.

\section{Problem 5}

```{r message=FALSE}
# setwd('~shifan/Dropbox/STANFORD/15Winter/STATS315a/hw3/wine')

train <- read.csv("vcdata.csv")
N = nrow(train)
attach(train)
```

We construct natural spline

```{r}
library(splines)
model <- lm( train$y~ ns(train$t, df = 5, intercept=TRUE) + 
                      ns(train$t, df = 5, intercept=TRUE):x.1 + 
                      ns(train$t, df = 5, intercept=TRUE):x.2 -1)
```

Next we make predictions using the model

```{r}
newx <- data.frame(t=train$t, x.1 = rep(1,N), x.2 = rep(1,N))
pred <- predict(model, interval="confidence", type="terms", newdata = newx)
```

Now we plot the coefficients along with confidence interval

```{r, echo=FALSE}
matplot(train$t, cbind(pred$fit, pred$lwr, pred$upr),
        lty=c(rep(1,3),rep(2,6)),col=rep(1:3,3),
        type="l",xlab="t",ylab="Coefficients")
legend("topright",legend=c(expression(beta[0]),expression(beta[1]),expression(beta[2])),lty=1,col=1:3)
```

\clearpage
\section{Stats 315 Data Challenge: Quality Prediction of Portuguese Wine}
\subsection{Data Pre-processing}

In this problem, we are given a dataset of Portuguese wines with containing wine properties (e.g."color", "citric.acid", "pH") and their "qualitiy", an integer evaluated by wine experts.
We aim to predict "quality" of Portuguese wines based on the provided properties of each wine.
We decide to formulate this prediction process as a supervised learning problem.
In particular, we use the provided training data to find the relavant properties of wines and their correlations with the wine "quality".
From the according correlations, we build a supervised learning model to predict "quality" based on unseen values of the predictors.

To achieve this, here is an outline the different methods we are going implement

\begin{itemize}
\item Linear regression (with regularization). We will build a linear model based on original features and try to predict wine "quality" based on the model. To find an optimal linear model, we also penalize the regression coefficients with regularization. Cross-validation will be used to evaluate the linear models.
\item Subset selection. We will find the dependence of prediction error rate on feature selection by subset selection. Forward stepping method and exhaustive search methods will be used.
\item Principle component analysis. Original data will be transformed into principle components. A visualization of the principle components will help find the correlation between different features and wine "quality". The principle components will also be used to build regression models.
\item Basis expansion. Interactions and transformations of original feature sets will be considered for potential improvements of the models in predicting wine "quality".
\end{itemize}

We start with first loading the relavant packages and reading trainig and test data

```{r message=FALSE}
library(MASS)
library(genridge)
library(mcmc)
library(glmnet)
library(boot)
library(leaps)
set.seed(23)
```

```{r}
setwd('~shifan/Dropbox/STANFORD/15Winter/STATS315a/hw3/wine')

# Load training and test samples
train <- read.csv("wine.train.csv", header=TRUE)
test <- read.csv("wine.test.ho.csv", header=TRUE)
```

We first visualize the correlations of each pair of properties and "quality" by generating a scatter plot of each possible pair.

![Pair scatter plots of features and response("quality"")](pairs.png)

```{r, echo=FALSE}
# pairs(train)
```

As we can see, since the first feature (labeled "X") is clearly only a label in the order of wine color (see the correlat), we can omit the first feature. Moreover, within the predictors we have both categorical feature (color) and numerical features. Therefore we first encode the categorical data (only two categories). 

```{r}
# Encode categorial data
train <- train[,2:ncol(train)]
test <- test[,2:ncol(test)]

train.color <- train[,1]
test.color <- test[,1]

train[,1] <- as.integer(model.matrix(~train.color)[,2])
test[,1] <- as.integer(model.matrix(~test.color)[,2])
p = ncol(train) - 1; # Number of features
```

\subsection{Linear regressions (and with regularization)}

We first implement ordinary linear regression without regularization. In particular, $k$-fold cross validation is used to evaluate the error rate of the regression model.

```{r}
# Linear regression
model <- glm(quality~., data=train)
cv.err <- cv.glm(train, model, K = 10)$delta
```

```{r, echo=FALSE}
cat('CV-MSE using Linear regression =',cv.err[2])
```

Make prediction with linear regression and write to file

```{r}
pred_lm <- predict(model,test)
write(pred_lm,file="lm.txt",sep="\n")
```

Next we implement ridge regression. We plot the change of regression coefficients versus effective degree of freedom.

```{r}
# Ridge regression
lambda <- 10^seq(-5,5,0.1)
model <- ridge(train$quality~., train[,1:p], lambda = lambda)
```

```{r, echo=FALSE, fig.width = 10, fig.height = 10}
traceplot(model,X="df")
```

To further investigate the role of regularization, we evaluate the 10-fold cross-validation mean-squared error (MSE) of regularized regression using L1 (lasso)

```{r}
# note default number of folds in cross-validation is 10 in package cv.glmnet
lm.ridge <- glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], lambda=lambda, alpha=0)
cv.ridge.err=cv.glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], lambda=lambda, alpha=0)
```

```{r, echo=FALSE, fig.width = 10, fig.height = 5}
plot(cv.ridge.err)
cat('CV-MSE using Ridge Regression =',
    cv.ridge.err$cvm[cv.ridge.err$lambda==cv.ridge.err$lambda.min])
```

and L-1 norm penalties (lasso).

```{r}
lm.lasso <- glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], alpha=1)
cv.lasso.err <- cv.glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], alpha=1)
```

```{r, echo=FALSE, fig.width = 10, fig.height = 5}
plot(cv.lasso.err)
cat('CV-MSE using Lasso Regression =',
    cv.lasso.err$cvm[cv.lasso.err$lambda==cv.lasso.err$lambda.min])
```

As shown from the above comparisons, lasso and ridge regressions do not provide improved models with lower CV-MSE than linear regression without regularization.

```{r}
lasso.opt <- glmnet(model.matrix(~ ., train[,1:p]),train[,p+1], lambda=10^(-5),alpha=1)
pred_lasso <- predict(lasso.opt,newx=model.matrix(~.,test))
write(pred_lasso,file="lasso.txt",sep="\n")
```

\subsection{Subset selection}

In order to examine the dependence of MSE on subset size (of features), we use subset selection to find how MSE varies with diffferent selection of feature subsets. In particular, we start with linear regression model without regularization.

```{r}
# method='forward'
method='exhaustive'
subset.error <- array(0,c(1,p))
for (subset.size in 1:p){
  subset.choose <- summary(regsubsets(quality~., data=train,nvmax=subset.size,method=method))
  subset.best <- subset.choose$which[subset.size,2:p+1]
  subset = train[,c(which(as.numeric(subset.best)==1),p+1)]
  
  lm <- glm(quality~., data=subset)
  cv.err <- cv.glm(subset, lm, K = 10)$delta
  subset.error[subset.size] <- cv.err[2]
}
```

```{r, echo=FALSE, fig.width = 6, fig.height = 3}
plot(1:p,subset.error,type="o",xlab="Subset Size", ylab="CV Error")
cat('CV-MSE using subset selection (full model) =',subset.error[p])
```

As shown in the previous figure, cross-validation (CV) error of subset selection mostly monotonically decreases (except a local minimum at subset size 7 features) with subset size. CV error achieves minimum when all features are used. We attribute this to the limited size of feature space and large dataset. Therefore we conclude that all features will need to be used for predictions.

\subsection{Principle Components Analysis}

Next to elucidate the role of each feature in predicting the wine "quality", we perform principle component analysis to look at the correlations of each feature and "quality".

```{r}
pca <- princomp(train, scale=TRUE, cor=TRUE)
summary(pca)
```

```{r, echo=FALSE, fig.width = 6, fig.height = 3}
plot(pca,type="line")
```

We fist note that the first two principle components are most dominant after transformation. Therefore it is reasonable to look at the first two comonents of the features and responses.

```{r warning=FALSE, echo=FALSE, message=FALSE}
library(ggbiplot)
plot.pca <- ggbiplot(pca, obs.scale = 1, var.scale = 1, groups=train.color,
              varname.size = 3.7, labels.size=10, varname.adjust=2,
              ellipse = TRUE, circle = TRUE,alpha_arrow = 1)
plot.pca <- plot.pca + ylim(-5, 5) + xlim(-7,7)
```

```{r, echo=FALSE, warning=FALSE, fig.width = 6, fig.height = 6}
print(plot.pca)
```

Similar to what we found in ridge regression, the feature "alcohol" is positively correlated with "quality" Additionally, the features "density" and "volatile.acidity" are negatively correlated with "quality".

Next we use the principle components of the predictors and use regression model to predict "quality".

```{r}
train.pca <- princomp(model.matrix(~ ., train[,1:p]), scale=TRUE, cor=TRUE)
lm.pca.ridge <- glmnet(train.pca$scores,train[,p+1], alpha=0)
cv.pca.ridge.err=cv.glmnet(train.pca$scores,train[,p+1], alpha=0)
```

```{r, echo=FALSE, fig.width = 10, fig.height = 5}
plot(cv.pca.ridge.err)
cat('CV-MSE using PCA Ridge Regression =',
    cv.pca.ridge.err$cvm[cv.pca.ridge.err$lambda==cv.pca.ridge.err$lambda.min])
```

\subsection{Support-Vector Machines}

```{r}
svm <- svm(model.matrix(~ ., train[,1:p]),train[,p+1],
           scale = FALSE, kernel = "radial", cross=10)
```

```{r, echo=FALSE}
cat('Cross Validation Error using SVM =',svm$tot.MSE)
```

\subsection{Basis expansion}

From the PCA analysis, we observed correlations between some predictors. Therefore we consider the interactions between predictors. We start with linear regression with interactio terms (2nd order interactions).

```{r}
model <- glm(quality~.^2, data=train)
cv.err <- cv.glm(train, model, K = 10)$delta
cat('CV-MSE using Linear regression =',cv.err[2])
```

Next we consider ridge regression, again with 2nd order interactions.

```{r}
# note default number of folds in cross-validation is 10 in package cv.glmnet
lm.int.ridge <- glmnet(model.matrix(~ .^2, train[,1:p]),train[,p+1], lambda=lambda, alpha=0)
cv.int.ridge.err=cv.glmnet(model.matrix(~ .^2, train[,1:p]),train[,p+1], lambda=lambda, alpha=0)
```

```{r, echo=FALSE, fig.width = 10, fig.height = 5}
plot(cv.int.ridge.err)
cat('CV-MSE using Ridge Regression =',
    cv.int.ridge.err$cvm[cv.int.ridge.err$lambda==cv.int.ridge.err$lambda.min])
```

and write to output

```{r}
ridge.int.opt <- glmnet(model.matrix(~ .^2, train[,1:p]),train[,p+1], lambda=cv.int.ridge.err$lambda.min,alpha=0)
pred.ridge.int <- predict(ridge.int.opt,newx=model.matrix(~.^2,test))
write(pred.ridge.int,file="ridge_int.txt",sep="\n")
```

Now consider lasso regression with interaction terms.

```{r}
# note default number of folds in cross-validation is 10 in package cv.glmnet
lm.int.lasso <- glmnet(model.matrix(~ .^2, train[,1:p]),train[,p+1], lambda=lambda, alpha=1)
cv.int.lasso.err=cv.glmnet(model.matrix(~ .^2, train[,1:p]),train[,p+1], lambda=lambda, alpha=1)
```

```{r, echo=FALSE, fig.width = 10, fig.height = 5}
plot(cv.int.lasso.err)
cat('CV-MSE using Ridge Regression =',
    cv.int.lasso.err$cvm[cv.int.lasso.err$lambda==cv.int.lasso.err$lambda.min])
```

and write to output

```{r}
lasso.int.opt <- glmnet(model.matrix(~ .^2, train[,1:p]),train[,p+1], lambda=cv.int.lasso.err$lambda.min,alpha=1)
pred.lasso.int <- predict(lasso.int.opt,newx=model.matrix(~.^2,test))
write(pred.lasso.int,file="lasso_int.txt",sep="\n")
```